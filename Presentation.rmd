---
title: "Living Well Service users - EDA and Cluster Analysis"
author: "Nujcharee Haswell"
date: "7 February 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Data Science Accelerator
========================================================
author: Nujcharee Haswell
date:  08/02/2018
autosize: true

About Living Well Service
========================================================

- Living Well North Yorkshire is a service which aims to support individuals in making changes to their lives. It forms part of the council's wider prevention programme. 

- The service offers Living Well Co-ordinators, who work with individuals including carers who may be on the cusp of needing formal care, by visiting their homes on a one-to-one basis. 

- Our aims are to work with individuals to help them identify solutions to address their health and wellbeing goals and maximise period of indepedence.


Challenges
========================================================
Over 4,000 contacts made since 2015. Everyone is offered access to services if they havent already received formal social care or havent known to the social care assessments / reviews process. 

- Identify characteristics of service users who likely will benefit most from the service (consequently do not require social care pathway, periods of independence is maximised)

- Identify the likihood whether they are to return for social care services within the next 12 months.


How we are going about to achieve this
========================================================
Apply Machine Learning techniques:


- Unsupervised Machine Learning -> KMeans Cluster Analysis
- Supervised Machine Learning -> General Linear Regression (GLM) for binary classification 


Data preparation
========================================================
To achieve the above objectives, my initial thoughts were to follow below approaches.
I use R for this project:

- Unsupervised Machine Learning -> KMeans Cluster Analysis
- Supervised Machine Learning -> General Linear Regression (GLM) for binary classification 

Exploring data

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(readr)
suppressWarnings(library(ggplot2))

lwa <- read.csv("C:/viz/Accelerator/data/train.csv", stringsAsFactor = FALSE, na.strings =  c("null", ""))

lwa$PersonID = NULL

lwa = lwa[,1:10]
head(lwa)

##find number of records
dim(lwa)

##1,000 random sample records are loaded into the workspace with 58 columns

train <- read.csv("C:/viz/Accelerator/data/Book2.csv", stringsAsFactor = FALSE, na.strings =  c("null", ""))

```



Dealing with missing data
========================================================
```{r warning=FALSE, message=FALSE}
train$PersonID = NULL
Missing_indices <- sapply(train,function(x)sum(is.na(x)))
Missing_Summary <- data.frame(index = names(train),Missing_Values=Missing_indices)
Missing_Summary[Missing_Summary$Missing_Values > 0,]

Missing_m = data.matrix(Missing_Summary)
Missing_m
missing_heatmap <-  heatmap(Missing_m, Rowv=NA, Colv=NA, col = cm.colors(256), scale="column", margins=c(3,3))

```

As demonstrated in the heatmap, there are a number of columns that contain missing values. 
I got a couple of approaches to tackle 

- Approach #1 -> Remove records
- Approach #2 -> Imputation technique application called "Label encoding" which is simply converting each value in a column to a number.


Feature Selections 
========================================================
Visualising Data
- To compare between two groups - Repeating Service Users / Non - Repeating

```{r, echo=FALSE}
train <- read.csv("C:/viz/Accelerator/data/Book1.csv", stringsAsFactor = FALSE, na.strings =  c("null", ""))

train1 <- read.csv("C:/viz/Accelerator/data/Book2.csv", stringsAsFactor = FALSE, na.strings =  c("null", ""))

train1$Repeating = as.factor(train1$Repeating)
train$Repeating = ifelse(train$Repeating==1,"Repeating", "Independence")
train1$Repeating = ifelse(train1$Repeating==1,"Repeating", "Independence")
train$Repeating = as.factor(train$Repeating)


train1$PersonID = NULL
train$PersonID = NULL
train$IMDScore = NULL



train1$Index.of.Multiple.Deprivation.Decile = ifelse(is.na(train1$Index.of.Multiple.Deprivation.Decile),median(train1$Index.of.Multiple.Deprivation.Decile),train1$Index.of.Multiple.Deprivation.Decile)

par(mfrow=c(2,2))

g = ggplot(data = train, aes(x=AgeToday, colour=Repeating)) +
  geom_histogram(aes(y=..density.., fill=Repeating), alpha=.3) +
  geom_density(size=1.5) +
  facet_grid(factor(Repeating) ~ .) +  ggtitle("Relationship between Age and User Groups")

g

g = ggplot(data = train1, aes(x=Index.of.Multiple.Deprivation.Decile, colour=Repeating)) +
  geom_histogram(aes(y=..density.., fill=Repeating), alpha=.3, bins = 10) +
  geom_density(size=1.5) +
  facet_grid(Repeating ~ .) +  ggtitle("Relationship between Deprivation Index and User Groups (10 = Least Deprived)")

g
```

##Decision Tree with RPart package 

Since there are 50+ variables and this is a classification problem,  I look for a technique that helps me explore data visually.


```{r warning=FALSE, message=FALSE}
library(rpart)
library(rpart.plot)
library(caret)
library(visNetwork)

ints <- sapply(train, is.integer)
intsCols <- names(train[, ints])


# data = train
# 
# data[intsCols] = gsub(0,"No",data[intsCols])
# data[intsCols] = gsub(1,"Yes",data[intsCols])

##data %>% mutate_at(intsCols, funs(factor(.)))



tree = rpart(Repeating ~. - AgeToday, data = train, method = "class")
summary(tree)


##Explore variable importance using Caret
varImp(tree)
prp(tree, extra=1, faclen=0,  nn=T, box.palette="Blues")

visTree(tree, main = "Living Well Service Users Tree", width = "100%")

```



## Decision tree Pros & Cons

Advantages
- Simple to understand, interpret, visualise (small)
- Can handle both numerical and categorical features
- Can handle missing data (automatically)
- Robust to outliers
- Require little data preparationa

Disadvantage

- Large trees can be hard to interpret
- Trees have high variance, which causes model to performance to be poor
- Trees overfit easily



## Lessons Learned + Next steps

- Spend more time understanding the data e.g format of data, is it appropriate for this analysis
- Is the data at the right granuarity? Person Level, Contact Level
- Revise ethnical framework by GDS
- Focused!!!

Next step -> explore predictive modelling


## More about RPart

http://oldemarrodriguez.com/yahoo_site_admin/assets/docs/Ploting_a_Tree_with_prp.234140400.pdf

```{r, echo=FALSE}
heat.tree <- function(tree, low.is.green=FALSE, ...) { # dots args passed to prp
y <- tree$frame$yval
if(low.is.green)
y <- -y
max <- max(y)
min <- min(y)
cols <- rainbow(99, end=.36)[
ifelse(y > y[1], (y-y[1]) * (99-50) / (max-y[1]) + 50,
(y-min) * (50-1) / (y[1]-min) + 1)]
prp(tree, branch.col=cols, box.col=cols, ...)
}

heat.tree(tree, type=4, varlen=0, faclen=0, fallen.leaves=TRUE)

```